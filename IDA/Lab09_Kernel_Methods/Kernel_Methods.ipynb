{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWgH_sxklpBQ"
      },
      "source": [
        "# Kernel Methods (Primal vs. Dual View)\n",
        "\n",
        "In this lab we explore how kernel methods can be used on structured data as long as a kernel function can be defined on pairs of objects of data. Specifically, we will use the dynamic time-warping (DTW) kernel to perform learning on sequences. We then proceed to train a kernelized SVM with the DTW kernel on a sequence data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OlZErOsYlpBS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5rcWIJXlpBT"
      },
      "source": [
        "## 1. DTW Kernel\n",
        "\n",
        "Given a metric $d: X \\times X \\rightarrow \\mathbb{R}_{\\geq 0}$ on the input space $X$, the family of *DTW Kernels* is given as:\n",
        "\n",
        "$$ k_{\\text{DTW}}(x, x') = e^{- \\lambda d_{\\text{DTW}}(x, x'; d)}, $$\n",
        "\n",
        "for sequences $x, x' \\in X^+ := \\bigcup_{n \\geq 1}{X^n}$ of lengths $|x|$ and $|x'|$. The *DTW distance metric* $d_{\\text{DTW}}$ is then given by $\\gamma(|x|, |x'|)$, where the helper function $\\gamma$ is defined recursively via:\n",
        "\n",
        "$$ \\gamma(i, j) = \\begin{cases} d(x_i, x_j') + \\min\\left(\\gamma(i-1, j-1), \\gamma(i-1, j), \\gamma(i, j-1)\\right) & (1 \\leq i \\leq |x|, \\, 1 \\leq j \\leq |x'|), \\\\ \n",
        "\\infty & i = 0 \\vee j = 0, \\\\\n",
        "0 & (i, j) = (0, 0). \\end{cases}\n",
        "$$\n",
        "\n",
        "The intuition is that $\\gamma(i, j)$ is the minimum squared distance up to time $i$ and $j$. $i = |x|$ and $j = |x'|$ are edge cases in the sense that the if a sequence has ended it cannot be matched anymore (and thus the value are infinite or the result value as both have been matched).\n",
        "\n",
        "To compute $d_{\\text{DTW}}$ the technique of <a href=\"https://en.wikipedia.org/wiki/Dynamic_programming\" target=\"_blank\">Dynamic Programming</a> is being used, where you store $\\gamma$ in a $(|x|+1) \\times (|x'|+1)$ grid.\n",
        "\n",
        "<b>Exercise 1</b>:\n",
        "\n",
        "Implement the function *d_DTW(x, x2, dist)*. The inputs x and x2 are the sequences to be compared and the parameter dist is a function on a pairs of points of the input space $X$ that outputs a real number (the distance between the pairs of points). Some code is given to help you dealing with the edge cases. The function is supposed to return the value of $d_{\\text{DTW}}$ (The distance between x and x2) with the specified parameters, *not* the $k_{\\text{DTW}}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gYj0jSc1lpBU"
      },
      "outputs": [],
      "source": [
        "def d_DTW(x, x2, dist):\n",
        "    t1, t2 = len(x), len(x2)\n",
        "    \n",
        "    # Edge Cases\n",
        "    if x == [] and x2 == []:\n",
        "        return 0.0\n",
        "    elif (x == []) or (x2 == []):\n",
        "        return np.infty\n",
        "    \n",
        "    dp = np.empty((t1 + 1, t2 + 1))    \n",
        "    dp[0, 0] = 0    \n",
        "    \n",
        "    for i in range(1, t1 + 1):\n",
        "        dp[i, 0] = np.infty\n",
        "    \n",
        "    for j in range(1, t2 + 1):\n",
        "        dp[0, j] = np.infty\n",
        "        \n",
        "    # Standard Procedure\n",
        "    for i in range(1, t1 + 1):\n",
        "        for j in range(1, t2 + 1):\n",
        "            cost = dist(x[i - 1], x2[j - 1])\n",
        "            dp[i, j] = cost + min(dp[i - 1, j - 1], dp[i - 1, j], dp[i, j - 1])\n",
        "    \n",
        "    return dp[t1, t2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6X3t8WLlpBU"
      },
      "source": [
        "Check your solution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zjGiKQaGlpBV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There is no error in your function!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    assert d_DTW([1, 2, 3, 3], [1, 2, 3], lambda x, y: 1 if x != y else 0) == 0.0\n",
        "    assert d_DTW([1, 2, 3, 4], [1, 2, 3], lambda x, y: 1 if x != y else 0) == 1.0\n",
        "    assert d_DTW([1, 2, 3, 2], [1, 2], lambda x, y: 1 if x != y else 0) == 1.0\n",
        "    assert d_DTW([], [1, 2], lambda x, y: 1 if x != y else 0) == np.infty\n",
        "    assert d_DTW([], [], lambda x, y: 1 if x != y else 0) == 0.0\n",
        "    print(\"There is no error in your function!\")\n",
        "except AssertionError:\n",
        "    print(\"There is an error in your function!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXBUCHZLlpBW"
      },
      "source": [
        "We define three distance functions on two values $x, x' \\in X$:\n",
        "\n",
        "$d_1(x_2, x_2) = \\mathbb{1}[x_1 != x_2]$, (a boolean Value should be returned)\n",
        "\n",
        "$d_2(x_1, x_2) = (x_1 - x_2)^2$,\n",
        "\n",
        "$d_3(x_1, x_2) = |x_1 - x_2|$,\n",
        "\n",
        "Optional: $d_4(\\Delta x_i, \\Delta x'_i) = (\\Delta x_i - \\Delta x'_i)^2$, with\n",
        "$$ \\Delta x_i = \\frac{1}{2}\\left( x_i - x_{i-1} + \\frac{x_{i+1} - x_{i-1}}{2}\\right) $$\n",
        "as *approximate derivates of order 2*. Note that the edge cases are $\\Delta x_1 = 0$ and $\\Delta x_{|x|} = x_{|x|} - x_{|x|-1}$. \n",
        "\n",
        "*Hint*: It's best to map the sequences $x = (x_1, \\dots, x_{|x|})$ to $\\Delta x = \\left(\\Delta x_1, \\dots, \\Delta x_{|x|}\\right)$ and then apply $d_2$.\n",
        "\n",
        "<b>Exercise 2</b>:\n",
        "\n",
        "Implement the missing distance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "SgBZXuPWlpBX"
      },
      "outputs": [],
      "source": [
        "def d1(x1, x2):\n",
        "    return 1 if x1 != x2 else 0\n",
        "\n",
        "\n",
        "def d2(x1, x2):\n",
        "    return (x1 - x2) ** 2\n",
        "\n",
        "def d3(x1, x2):\n",
        "    return abs(x1 - x2)\n",
        "\n",
        "def delta_x(x):\n",
        "    n = len(x)\n",
        "    delta_x = [0] * n\n",
        "    if n > 1:\n",
        "        delta_x[0] = 0\n",
        "        delta_x[-1] = x[-1] - x[-2]\n",
        "        for i in range(1, n-1):\n",
        "            delta_x[i] = 0.5 * (x[i] - x[i-1] + (x[i+1] - x[i-1]) / 2)\n",
        "    return delta_x\n",
        "\n",
        "def d4(x, x2):\n",
        "    delta_x1 = delta_x(x)\n",
        "    delta_x2 = delta_x(x2)\n",
        "    n = len(delta_x1)\n",
        "    return sum((delta_x1[i] - delta_x2[i]) ** 2 for i in range(n))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-VgCjSylpBX"
      },
      "source": [
        "The following code lifts the distance metrics to maps that map a given hyperparameter $\\lambda$ return the corresponding kernel function $k_{\\text{DTW}}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "4et9GHKOlpBX"
      },
      "outputs": [],
      "source": [
        "k1_hyp, k2_hyp, k3_hyp = [lambda lmbd: (lambda x, x2: np.exp(-lmbd * d_DTW(x, x2, d))) for d in [d1, d2, d3]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "WpFwIgvdlpBX"
      },
      "outputs": [],
      "source": [
        "k1 = k1_hyp(2.0)\n",
        "k2 = k2_hyp(2.0)\n",
        "k3 = k3_hyp(2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjA3xhhVlpBY"
      },
      "source": [
        "The following code computes the Gram matrix $K$ with respect to the kernel $k$ (a parameter) and the data $xs$ (another parameter), see slide 28 and 29 in Kernel Methods lecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "fbeBxxnGlpBY"
      },
      "outputs": [],
      "source": [
        "def build_dtw_gram_matrix(xs, x2s, k):\n",
        "    \"\"\"\n",
        "    xs: collection of sequences (vectors of possibly varying length)\n",
        "    x2s: the same, needed for prediction\n",
        "    k: a kernel function that maps two sequences of possibly different length to a real\n",
        "    The function returns the Gram matrix with respect to k of the data xs.\n",
        "    \"\"\"\n",
        "    t1, t2 = len(xs), len(x2s)\n",
        "    K = np.empty((t1, t2))\n",
        "    \n",
        "    for i in range(t1):\n",
        "        for j in range(i, t2):\n",
        "            K[i, j] = k(xs[i], x2s[j])\n",
        "            if i < t2 and j < t1:\n",
        "                K[j, i] = K[i, j]\n",
        "        \n",
        "    return K\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "HHaPud46lpBZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.35335283e-01, 4.53999298e-05],\n",
              "       [4.53999298e-05, 2.47875218e-03]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "build_dtw_gram_matrix([[1, 2], [2, 3]], [[1, 2, 3], [4]], k1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F6dT70MlpBZ"
      },
      "source": [
        "## 2. Kernel SVM\n",
        "\n",
        "Now we implement the training algorithm for kernel SVMs. We adjust the ERM learning algorithm from the linear classification lab. First we are reusing the code for the $\\mathcal{L}_2$-regularizer and the hinge loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "spSeL8kJlpBZ"
      },
      "outputs": [],
      "source": [
        "def L2_reg(w, lbda):\n",
        "    return 0.5 * lbda * (np.dot(w.T, w)), lbda*w\n",
        "\n",
        "def hinge_loss(h, y):\n",
        "    n = len(h)\n",
        "    l = np.maximum(0, np.ones(n) - y*h)\n",
        "    g = -y * (h > 0)\n",
        "    return l, g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gDqL_BHlpBa"
      },
      "source": [
        "<b>Exercise 3</b>:\n",
        "\n",
        "Adjust the old code (Lab 06) to actually learn the kernel linear regression. Note that there is a new parameter $k$ that encodes the kernel function. Note that lbda is not the $\\lambda$ used in the definition of $k$, but the regularization coefficient (as before). Note also that the learning rate $\\alpha$ has been renamed to $\\eta$, because $\\alpha$ coincides with the dual coefficients (see lecture).\n",
        "Also make sure to return the Gram matrix $K$ together with the weight vector $w$ (or $\\alpha$), as it is costly to compute and needed for the inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "id": "6Zyj6AxClpBa"
      },
      "outputs": [],
      "source": [
        "def learn_reg_kernel_ERM(X, y, lbda, k, loss=hinge_loss, reg=L2_reg, max_iter=200, tol=0.001, eta=1., verbose=False):\n",
        "    \"\"\"Kernel Linear Regression (default: kernelized L_2 SVM)\n",
        "    X -- data, each row = instance\n",
        "    y -- vector of labels, n_rows(X) == y.shape[0]\n",
        "    lbda -- regularization coefficient lambda\n",
        "    k -- the kernel function\n",
        "    loss -- loss function, returns vector of losses (for each instance) AND the gradient\n",
        "    reg -- regularization function, returns reg-loss and gradient\n",
        "    max_iter -- max. number of iterations of gradient descent\n",
        "    tol -- stop if norm(gradient) < tol\n",
        "    eta -- learning rate\n",
        "    \"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    \n",
        "    g_old = None\n",
        "    \n",
        "    K = np.zeros((n_samples, n_samples))\n",
        "    for i in range(n_samples):\n",
        "        for j in range(n_samples):\n",
        "            K[i, j] = k(X[i], X[j])\n",
        "\n",
        "    alpha = np.random.randn(n_samples)    \n",
        "    \n",
        "    for t in range(max_iter):\n",
        "        h = np.dot(K, alpha) \n",
        "        l,lg = loss(h, y)\n",
        "        \n",
        "        if verbose:\n",
        "            print('training loss: ' + str(np.mean(l)))\n",
        "            \n",
        "        r,rg = reg(alpha, lbda)\n",
        "        g = lg + rg \n",
        "        \n",
        "        if g_old is not None:\n",
        "            eta = 0.9**t\n",
        "            \n",
        "        alpha = alpha - eta*g\n",
        "        if (np.linalg.norm(eta*g)<tol):\n",
        "            break\n",
        "        g_old = g\n",
        "        \n",
        "    return alpha, K\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU19wUW8lpBa"
      },
      "source": [
        "The adjusted inference function is given as (for binary classification):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "DgGz5h7_lpBb"
      },
      "outputs": [],
      "source": [
        "def predict(alpha, X, X_train, k):\n",
        "    K = build_dtw_gram_matrix(X_train, X, k)\n",
        "    y_pred = np.dot(K, alpha)\n",
        "    y_pred[y_pred >= 0] = 1\n",
        "    y_pred[y_pred < 0] = -1\n",
        "    \n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ2KR7g7lpBb"
      },
      "source": [
        "## 3. DTW Kernel SVM in Action\n",
        "\n",
        "Now we put our results from section $1$ and $2$ together to use a kernelized SVM for a classification task on sequence data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "id": "oHJc04CDlpBb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50, 60) (50,)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from scipy.io import loadmat # for matlab *.mat format, for modern once need to install hdf5\n",
        "\n",
        "file_path = \"laser_small.mat\" # file path for multi os support\n",
        "mat = loadmat(file_path)\n",
        "\n",
        "X = mat['X']\n",
        "y = mat['Y'].reshape(50)\n",
        "\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYSDVVV1lpBb"
      },
      "source": [
        "We have only 50 training instances and thus only go for a simple train-test-split (we cannot afford a simple train-val-test-split). If we try several kernels, we are actually tuning a hyperparameter and thus are fitting on the test set. The solution to this problem would be the nested cross-validation procedure, which we learn in the evaluation lecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "id": "Z-sjThv4lpBc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(33, 60) (17, 60)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "id": "I2b-0g7SlpBc"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (60,) (0,) ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m alpha, K \u001b[38;5;241m=\u001b[39m \u001b[43mlearn_reg_kernel_ERM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlbda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[17], line 20\u001b[0m, in \u001b[0;36mlearn_reg_kernel_ERM\u001b[0;34m(X, y, lbda, k, loss, reg, max_iter, tol, eta, verbose)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[0;32m---> 20\u001b[0m         K[i, j] \u001b[38;5;241m=\u001b[39m \u001b[43mk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m alpha \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(n_samples)    \n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n",
            "Cell \u001b[0;32mIn[5], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x, x2)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m k1_hyp, k2_hyp, k3_hyp \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mlambda\u001b[39;00m lmbd: (\u001b[38;5;28;01mlambda\u001b[39;00m x, x2: np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mlmbd \u001b[38;5;241m*\u001b[39m \u001b[43md_DTW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m)) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m [d1, d2, d3]]\n",
            "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36md_DTW\u001b[0;34m(x, x2, dist)\u001b[0m\n\u001b[1;32m      2\u001b[0m t1, t2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x), \u001b[38;5;28mlen\u001b[39m(x2)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Edge Cases\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m x2 \u001b[38;5;241m==\u001b[39m []:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (x \u001b[38;5;241m==\u001b[39m []) \u001b[38;5;129;01mor\u001b[39;00m (x2 \u001b[38;5;241m==\u001b[39m []):\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (60,) (0,) "
          ]
        }
      ],
      "source": [
        "alpha, K = learn_reg_kernel_ERM(X_train, y_train, lbda=1, k=k2, max_iter=20000, eta=1, tol=1e-3, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkFyVohvlpBc"
      },
      "source": [
        "And evaluation of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-wWrtE1alpBc"
      },
      "outputs": [],
      "source": [
        "y_pred = predict(alpha, X_train, X_train, k2)\n",
        "print(\"Training Accuracy: {}\".format(np.mean(y_train == y_pred)))\n",
        "print(\"Test Accuracy: {}\".format(np.mean(y_test == predict(alpha,X_train, X_test, k2))))\n",
        "print(\"Shape of alpha {}\".format(alpha.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWzZYVWGlpBd"
      },
      "source": [
        "We see that the training accuracy is far better than the test accuracy. This *could* - but does not have to - mean that we are overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zdnO6velpBd"
      },
      "source": [
        "Vary the choices of the kernel functions, regularization parameters and kernel smoothing parameters (the $\\lambda$ in the definition of $k_{\\text{DTW}}$). In the rest of the notebook you learn how you can draw learning curves we have discussed in the tutorial. To be able to use the helper function, the estimator needs to be wrapped in a scikit-learn conform way. You can find and use the example class KernelEstimator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ubgy85AAlpBd"
      },
      "outputs": [],
      "source": [
        "#from sklearn.learning_curve import learning_curve\n",
        "from sklearn.model_selection import learning_curve\n",
        "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
        "                        n_jobs=1,\n",
        "                        train_sizes=10, # list of floats that describe ratio of test data sets tried\n",
        "                        # OR an int = # how many trials\n",
        "                        scoring=None):\n",
        "\n",
        "    if type(train_sizes) == int:\n",
        "        train_sizes=np.linspace(.1, 1.0, train_sizes)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        " \n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    if cv is not None:\n",
        "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    \n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "    if cv is not None:\n",
        "        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation score\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qpp3Z1wPlpBd"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class KernelEstimator(BaseEstimator):\n",
        "    \n",
        "    def __init__(self, k, lbda):\n",
        "        self.k = k\n",
        "        self.lbda = lbda\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        self._X_train = X\n",
        "        self._alpha, _ = learn_reg_kernel_ERM(X, y, lbda=self.lbda, k=self.k, max_iter=20000, eta=1, tol=1e-3)\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return predict(self._alpha, self._X_train, X, self.k)\n",
        "    \n",
        "    def score(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        return np.mean(y == y_pred)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkL1wv7rlpBd"
      },
      "source": [
        "<b>Exercise 4:</b>\n",
        "\n",
        "Vary the choices of the kernel functions, regularization parameters and kernel smoothing parameters (the $\\lambda$ in the definition of $k_{\\text{DTW}}$). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muEMMK_xlpBe"
      },
      "outputs": [],
      "source": [
        "estimator = KernelEstimator(k2_hyp(2.0), 2.0)   # MODIFY \n",
        "estimator.fit(X_train, y_train)\n",
        "print(\"Accuracy {}\".format(estimator.score(X_train, y_train)))\n",
        "plot_learning_curve(estimator, 'Euclidean distance DTW, lambda = 2.0', X, y, cv=3, scoring=\"accuracy\", train_sizes=[0.01,0.1,0.3,0.5,0.6,0.7,0.8,0.9,1.0]);"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Kernel_Methods.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
